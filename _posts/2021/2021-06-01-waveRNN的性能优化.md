---
layout: post
title: 【wavernn与性能优化】wavernn与性能优化
categories: vocoder 语音合成
description: wavernn与性能优化
keywords: vocoder, 语音合成
---

之前一直打算学习一下强化学习，也看了一些教程，比如西瓜书《机器学习》上的强化学习教程，但学完发现公式非常复杂，给人一种非常痛苦的感觉，再比如莫凡强化学习教程，一开始就有点云里雾里的感觉，感觉这个教程是需要一定的强化学习基础，再去学就好多了。

最近找到了口碑不错的一个教程，是出自David Sliver的，这位大神是来自DeepMind团队的，教程深入浅出，是我目前看到的入门强化学习最好的教学，教学资源请点击[这里](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)，里面包括ppt和英文视频，网友也有[中文版](https://www.bilibili.com/video/av32149008?from=search&seid=9588183746086039596)的视频翻译，不过有些地方翻译的不太准确，不过配合ppt看的话，也还能看。如果你想学习这份教程，我有以下几点建议：

- 如果你的英文水平不错的话，建议直接观看原英文视频，并在看完是视频后过一遍ppt

我终于用了一个星期左右的时间看完了这个教程，这里相当于是一个学习笔记，主要内容是按照这个教程的ppt走的，可能会加一些自己的理解，如果有哪里不对的欢迎讨论。

# 简介
强化学习在不同的领域有不同的表现形式：

## 奖励(Reward)
- 奖励是一个标量反馈信号
- $R_t$表明代理(Agent)在第t步做的有多好
- 代理的工作就是最大化累积奖励(也就是我最大化我的整个过程中的奖励和，而不是最大化某一步的奖励)

奖励假设的定义：强化学习中所有的目标都可以描述为最大化累积奖励的期望。奖励假设是强化学习的根基。

序列决策的制定：

- 目标：选择行为来最大化未来总奖励
- 行为可能有长期的影响
- 奖励可能延迟
- 有时牺牲短期的奖励会更好，以此来获取长期的奖励

迷宫的例子中每走一步的奖励都是-1，因为我们想通过最少的格子走出迷宫，比如通过十个格子走出，比通过十一个格子走出要好，我们通过这种奖励设定来鼓励代理寻找最优的策略。

>这是个特例，其他的例子中奖励可能会根据不同的状态下作出不同的动作而有所不同，并不全是-1。有的任务中奖励可能是明确的，比如打飞机游戏中，打掉一个飞机多少分数，打掉一个boss多少分数，而这个奖励以及打掉飞机和打掉boss的奖励相对大小都是根据游戏的宗旨(努力存活更久)制定的，而现实中我们要解决的任务中，可能需要我们自己去根据任务目标和不明确的反馈来制定奖励函数。这里举一个例子，比如百度搜索排序任务中，用户在搜索结果中点击排在第一的item和点击排在第二的item奖励显然不同，或者点击搜索结果第二页的item，奖励也不同，这就需要我们根据这些不明确的反馈来制定奖励函数。后面有机会再介绍工业上强化学习的运用。


## 代理(Agent)和环境(Environment)
强化学习的过程实际就是一个代理和环境交互的过程。

在第t步，你：

- 执行动作(向上，向下，向左，向右)
- 收到观察(现在所处的位置)
- 收到奖励(-1)

网格环境：

- 收到动作指令
- 呈现观察(比如向上，就呈现上移一格的观察)
- 反馈给你奖励(每走一步-1)

## 历史(History)和状态(State)
**历史**指的是观察(Observation)、动作(Action)、奖励(Reward)的序列

$$H_t = O_1, R_1, A_1,..., A_{t-1}, O_t, R_t$$

**状态**包含来自历史中有用的信息，正式的讲，状态是历史的函数：

$$S_t = f(H_t)$$

由于环境的是否可完全观测，状态分为完全观察状态和部分观察状态。

比如迷宫的例子中，我们先区分一下历史(History)、观察(Observation)、状态(State)，这里观察指的是位置，我们用一个元组(行数，列数)表示，比如start位置观察为(2,1)，这里奖励为-1，这里历史指的是我们走了一段路程后

$$H_t = (2,1), -1, 向右$$

$$H_{t+1} = (2,1), -1, 向右, (2,2), -1, 向上$$

$$H_{t+2} = (2,1), -1, 向右, (2,2), -1, 向上, (1,2), -1, 向右$$

那么状态呢？因为状态是历史的函数，对于第t+2步，我们可以说

$$S_{t+2} = (1,2)$$

或者

$$S_{t+2} = (2,1), (2,2), (1,2)$$

# 学习(Learning)和规划(Planning)
我们在序列决策制定的时候有两个基础的问题：

- 强化学习：

    - 环境模型起初是不了解的
    - 代理通过与环境交互来了解环境或者说获得反馈状态和奖励
    - 代理不断改善它的策略
