vc-


### 上半场回顾

2021-04～2021-07

最开始接触vc是在21年4月的时候，大师说要做一套vc系统，我就在很短的时间内攒了一个。
当时为了快速验证，采用了ctc（subsample=4）+ enc（cbhg）+ attention+decoder+hifigan的方案。
虽然流程比较长，但基本都是我们已经有的模块。虽然只是一个快速验证方案，我们还是经过了一些思考和推敲，因为采用asr-tts-based系统+单音色使用场景，音色相似度基本有保障，流畅度和可懂度大体觉得可以追平tts的目前效果。事实上也大致是这样。这样 vc 面临的挑战大体上就只剩下了鲁棒性。为了测试鲁棒性，当时花了不少时间来测试各种各样的case，发现我们的效果其实整体还可以，badcase高发区是快语速和带躁场景这两块，在目前的系统架构上，我们又做了一些小的改进，来规避这两方面的问题。

大概打磨了一个月后，发现目前的效果还不错，就梳理了全流程的推理过程，增加了vad等前端模块。同时，和建坤一起完成了demo页面，可以实时测听体验，也上线了两个音色：zhiling and wukong，大体上 vc 看上去有了个产品的样子。过程中间还做了一些细节零散的优化，略过不提。

后来去和搜狗线上的 vc 效果对比（他们刚好也有这两个音色），总体上感觉旗鼓相当，他们的鲁棒性更好一点，但我们相似度更高点。遇到比较难的case，可能他们还是出错的几率更少一点。

当时，信心满满跟大师保证再优化一下便可以上线，但没想到开始便是巅峰，好运到此戛然而止了。

上线前我们遇到了两个问题，一个是不能流式，一个是情感表达不好。看上去前者只要用支持chunk推理的网络单元，后者把pitch特征加进去就解决了，看上去似乎并不难，再不济可以解决其中一个。没想到进展比想象的更不顺利。

第一阶段，算法验证阶段，为了更快出结果，有几个地方处理的很草率。首先，speaker-embedding 一直没有奏效。可视化了deyi之前提供脚本抽取结果，不同说话人聚类效应明显，看上去效果很不错。但使用起来却不是很理想，在 baseline 上加上 spk-emb，带来负面效果。更多倾向于 spk-emb 使用方式待商榷，以及代码本身可能哪里写的有问题。如果单音色不用spk-emb，和多音色方案势必两套代码，感觉上会比较丑陋也不太利于后期维护。其次，ce based linguist embedding 和欣陶一起验证了很久，效果不如预期。ce-based-ling-embed 粒度更细，可以更好建模发音特征，也是比较主流的方案，但我们可能一直没处理好。我自己做了一个简单的版本，复现了剑涛之前的版本，找志平帮忙训练了kaldi的版本，（还有一个腾讯ai-lab的版本没有试），都没有取得更好的效果。主要表现在鲁棒性降低了，好的更好，差的更差了。看上去这里后面还是值得投入更多时间来看看原因和改进措施。

后来在创新项目评选中，大家提出了让vc能满足多方言和外文的需求，发现之前的baseline效果很不行。表现是在发其他语音时，还带有浓重的普通话的强调。目前的baseline方案，source 发音不清楚的时候，前者会把发音不清楚的字音，转换成相对标准的普通话发音。之前我一直觉得是个优点，但在方言转换任务里变成了缺点。这对我们提出了更高的要求。后来欣陶发现subsample=1是更好的方案，我们也在这方面着手改进。

关于 emotion-vc，之前的方案有个很大的缺点就是对于不同语调相同内容的source-audio，输出基本没有差异性，这显然和vc本来的出发点相违背。vc的输入信息必然包括内容信息以外更多的内容。一个自然的想法就如前文所说，把 pitch 特征引进来，但做了一下发现这个问题真的很难，貌似不是随便搞一下就能搞定的。主要难点有两个，流式场景下，pitch提取算法以及对噪声对鲁棒性，以及怎样pitch在source到target的偏移函数。虽然目前也有不少论文提到了一些解决方案，也大体上复现这些demo（例如西工大+爱奇艺的那篇），但距离实际使用还差的非常远。

### 中场休息

### 下半场回顾
202108～202120

回来之后，第一件事是先做一轮工程化。完全cpp化

