---
layout: post
title: 2022 Dec 论文笔记
categories: 读书笔记
description: 
keywords: 读书笔记
---
2022年12月 论文/读书笔记



## [SoundStream: An End-to-End Neural Audio Codec](https://arxiv.org/pdf/2107.03312.pdf)

Google 的神经网络codec 论文

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-12-27-papers-codec.png?raw=true" width="800" /></div>

几个要点: 
1. 神经网络声码器，纯端到端的方案。enc + quantizer + decoder 的 pipeline
2. residual quantizer, and 自适应bitrate（同一模型应对不同的比特率）
3. 高效的 encoder，可以用来代替 mel 
4. 更好的效果
5. 支持流式与低延时
6. 压缩和增强同时完成

模型层面，其实是一个类似 hifigan 的纯 Cnn 堆叠结构 + Gan 对抗训练。比较有意思的是 quantizer 的部分。 

## [AudioLM: a Language Modeling Approach toAudio Generation](https://arxiv.org/pdf/2209.03143.pdf)

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-12-28-papers-audioLM.png?raw=true" width="600" /></div>

Google 关于语音领域-预训练模型的论文

[论文对应demo页面](https://google-research.github.io/seanet/audiolm/examples/)

很有意思的想法。把连续的信号量，转化成semantic token，然后对semantic token进行建模(类似于 NLP 中的语言模型)。

我们一般都认为语音信号(和文本数据相比)是稀疏数据，高采样率下，存在过多的冗余表示。
这篇论文的做法，完成了对稀疏数据的稠密表达。和 ASR 之类的做法相比，semantic token 是一个比较折衷的方案，信息抽象程度和时间维度上，都介于信号声学特征和语义特征中间的状态。
同时，token 内也保留了一些语义的信息。
论文通过 encoder/decoder 结构，完成 signal to token 两个方向的转换。
decoder 部分相当于一个声码器，只是这里对应的特征从 mel 谱换成了之前提到的 semantic-token。
对于提取的 token，论文用一个 Mask-based model 来进行预测（类似bert，虽然我觉得可能使用类似 gpt 的模型架构会更好一点）。

这样两部分相结合，AudioLM 可以完成对 语音片段的预测。类似与文本预测+TTS，但这里不仅语义内容是连续可懂的，同时语气语调之类的副语言信息也能够很好的建模。

捎带手的，audioLM 解决了类似丢包补偿一类的音频信号传输的问题。以及做完一个大的 base-model，
应该在 tts 特别是 zero/one/few-shot 之类的场景下有比较优异的表现。也有可能在其他一些任务上，做完一个上游模块提供一些支持帮助。
但除此之外，他相比 HuBert/Wav2Vec2 之类的大模型优势在哪里？？
是否存在类似 chatGpt 之类吸引人注意，可以做到之前完全无法做到的事情？好像目前还没有想到。 

