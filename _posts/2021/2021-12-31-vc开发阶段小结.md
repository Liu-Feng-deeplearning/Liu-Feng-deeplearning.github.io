---
layout: post
title: vc算法阶段小结
categories: 语音信号处理
description: 
keywords: 语音信号处理
---


2021年，如果说总结自己在算法上的核心探索，可能 vc 就是其中重要一环。
希望能有一篇文章，讲下自己当时做的一些工作和经验，虽然可能并不完美，但做一个总结还是挺好的。

---

### 算法初探

最开始接触vc是在21年4月的时候，大师说要做一套vc系统，我就在很短的时间内攒了一套出来。
当时，采用了ctc（subsample=4）+ enc（cbhg）+ attention + decoder + hifigan 的方案。
因为都是我们已经有的模块，所以即使流程比较长，但搭建并不费事。
虽然这只是一个快速验证方案，我们还是经过了一些思考和推敲。
因为采用 asr-tts-based 系统+单音色使用场景，音色相似度基本有保障，流畅度和可懂度预期可以追平tts的目前效果。
这样 vc 面临的挑战大体上就只剩下了鲁棒性。
为了测试鲁棒性，当时花了一点时间来测试各种各样的case。
最后发现我们的效果其实整体还可以，给了我们很多信心，觉得要把 vc 这个事情持续的做下去。

当时，badcase 高发区是快语速和带躁场景这两块，在当时的系统架构上，我们又做了一些类似速度增强等小的改进，
来规避这两方面的问题。同时，我们给系统增加了 vad 等前端模块，让整个 vc 更像一个"系统"，而非"模型"。

与此同时，和建坤一起完成了demo页面，实时测听体验，同步上线了两个音色：zhiling and wukong，
大体上 vc 看上去有了个产品的样子。后来去和搜狗线上的 vc 效果对比（他们刚好也有这两个音色），
总体上感觉旗鼓相当，他们的鲁棒性更好一点，但我们相似度更高点。遇到比较难的case，可能他们还是出错的几率更少一点。

一时间信心满满，去找大师保证，再优化一下便可以上线，但没想到开始即是巅峰，好运到此戛然而止了。


上线前我们遇到了两个问题，一个是不能流式，一个是情感表达不好。
看上去前者只要用支持chunk推理的网络单元，后者把pitch特征加进去就解决了，似乎并不难，再不济可以解决其中一个。

没想到进展比想象的更不顺利。第一阶段，算法验证为了更快出结果，有几个地方处理的很草率。
首先，speaker-embedding 一直没有奏效。
可视化了 deyi 之前提供脚本抽取结果，不同说话人聚类效应明显，但使用起来却不是很理想。
在 baseline 上加上 spk-emb，总是会带来负面效果。
当时倾向于 spk-emb 使用方式待商榷，以及代码本身可能哪里写的有问题。
这个问题直到很久以后才解决掉，其实是多说话人数据丰富度不够导致的。
其次，ce-based or ctc-based ppg? 关于 linguist embedding 方案和欣陶一起验证了很久，
一直没什么头绪。ce 粒度更细，可以更好建模发音特征，也是比较主流的方案，但我们可能一直没处理好。
当时，我自己做了一个简单的版本（torch-tdnnf），复现了剑涛之前的版本（tdnn），找志平帮忙训练了kaldi的版本（cnn-tdnn），
还有一个腾讯ai-lab的版本（但受限与时间并没有试），都没有取得更好的效果。
主要表现在鲁棒性不理想，好的更好，差的更差了。
无奈我们最终选择了 ctc 的方案，还是先保证鲁棒性。
后来在创新项目评选中，大家提出了让vc能满足多方言和外文的需求，我们又发现之前的baseline效果很不行。
其表现是在发其他语音时，还带有浓重的普通话的腔调。
当时的方案下，source 发音不清楚的时候，前者会把发音不清楚的字音，转换成相对标准的普通话发音。
之前我一直觉得是个优点，但在方言转换任务里变成了缺点。
后来欣陶发现ctc下令subsample=1是更好的方案，后面我们也在这方面进行了一些尝试。
另外，值得总结的是，方法论上也存在一些问题，总是追求大模型/大数据，很多时间都浪费在无谓的实验上，并没有去探寻问题的本质和做出实质性的突破。
总而言之，ppg/content-embedding这块感觉做得还是比较粗糙，后续会把这块抽成一个单独的项目来做。

另一个想处理好却失败的点就是 emotion-vc。之前的方案有个很大的缺点，对于不同语调相同内容的source-audio，
输出基本没有差异性，这显然和 vc 本来的出发点相违背。vc 还是应该在语义本身传输完整性之外，包含更多节奏/预期/音色相关的内容。
尝试了一些基于 pitch/f0 特征的方法

一个自然的想法就如前文所说，把 pitch 特征引进来，但做了一下发现这个问题真的很难，貌似不是随便搞一下就能搞定的。
主要难点有两个，流式场景下，pitch 提取算法以及对噪声对鲁棒性，以及怎样 pitch 在 source 到 target 的偏移函数。
虽然目前也有不少论文提到了一些解决方案，我也大体上复现这些demo（例如西工大+爱奇艺的那篇），但距离实际使用还差的非常远。

关于流式模型，其实主要问题处在 ppg 的模型训练上，使用 wenet-transformer 训练流式模型，其实可以，包括 asr 组已经得到很不错效果的前提下，
发现我依然走了相当多的弯路。包括后来

### 工程优化与实现

工程层面，借着 vc 项目，着手搭建了更为通用的 HSS (Huya Streaming Speech) 平台，完善了整个流式的语音框架。
流式语音平台框架和工程相关内容，可能大多已经在前面文章中总结了，这里不赘述。
聊下这个带来的收获吧，可能这个项目带来的最大收获就是在大量的 c++ 开发下，对语言更熟悉了。
现在不会像之前那么发怵与写 c++。虽然 c++ 博大精深，远远谈不上精通，起码不会成为短板。
其次，对怎样落地一个工程项目也会了解熟悉更多，也可以更好的指导自己完成更好的算法开发。
例如，现在无论什么项目，都会尽量遵循音频进-音频出的模式，所有逻辑尽量都写在模型里，这样对工程实现的简便性有极大的帮助。
第三，就是了解了一些和服务层相关的知识，起码 websocket 开发有了一个入门级经验，也可以独立搭一些可用的服务出来。

和纯算法探索不同，工程层面还是有一些实打实的经验和成果积累下来，也算是2021下半场一个不错的成果吧。

### 反思与总结

首先就是上面提到的实验方法论的问题，过早的追求大模型or大数据，在无谓的实验上花费精力过多。
其次，对整体路线选择和把控还不太成熟。其实，可以先选择把离线方案做好，然后再搞在线方案是更稳妥的选择。
第三，很多关键的路线上，选择有待商榷。例如早点开始transformer，以及不要执迷于ctc-based ppg。
但这个更多和对算法的认知以及和对整个领域的scope有关，还需要慢慢积累。
第四，需要及时总结，代码不断重构和抽象，及时总结技术文档。这点也是之前几年工作中比较欠缺的点，可能今年才会逐渐好些。


### 下半场回顾
202108～202120

回来之后，第一件事是先做一轮工程化。完全cpp化

