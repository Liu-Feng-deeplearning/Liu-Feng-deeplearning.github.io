流式模型

语音算法中，流式模型是一个重要的应用场景。


流式场景是语音中特有的一类问题。和 vc 以及 nlp 问题不同，语音输入 or 输出具有极强的时序性，
音频是以 chunk 形式发送和接收的。因此，我们不需要等待所有 chunk 发送完毕，可以边发送边处理，
从而获得更好的实时性能和用户体验。

## 语音算法中的流式场景

这是几个常见的流式语音任务。

- VC(voice conversation), 音频进音频出。每次输入音频片段，输出转换后的音频。 
- ASR，音频进文本出。每次输入音频片段，输出该音频片段对应文本（可以为空）。
当所有音频输入结束时（输入对应结束标记），对已经输出所有文本进行重排or整合，得到最终输出文本。 
- TTS，文本进音频出。输入一段文本，依次输出每段生成音频，边播放边生成下一段音频。
- vad or AED，音频进标签出，每次输入音频片段，输出帧对应标签。

## 流式算法的平均指标

流式语音算法实现的前提是 rtf<1.0 即处理1s音频时所用时间小于1s，事实上由于还要考虑网络延时，rtf应该远小于1。
rtf满足要求的情况下，后续处理不会阻塞，在此前提下，我们通常使用如下两个指标来评估流式系统的性能。

- 首片延时，系统处理第一个音频片段时，从接收数据到返回结果所用的时间。该指标反映了流式系统下，
用户获得反馈的最小用时，是直观体现用户体验的核心指标。
- 每路服务cpu占用率，该指标反映了系统运行时对资源的占用情况。在多路并发场景下，该每路服务使用更少的cpu，
意味着相同资源下可以处理更多服务请求，从而节约了服务器成本。

在 asr 任务中，因为最后会对所有文本进行重排，因此我们有一个额外对测试指标：
- 尾片延时，服务接收到音频结束标志符到返回最终正确结果的延时，该指标意味着流式系统下拿到最准确asr结果所用延时，
也是反应asr系统性能的核心指标。在某些应用场景（例如客服对话系统）下，尾片延时是比首片延时更重要的指标参数。

## 流式算法的实现

如果一个算法是流式的，且和非流式在数值上完全相同，则需要满足(这里+表示concat的意思)

```math
f(chunk1+chunk2+chunk3...) = f(chunk1) + f(chunk2) + f(chunk3) ...
```

这样边可以对每个输入片段分别计算再拼接，从而达到流式效果。对于全连接层（大多针对非时间维度进行计算），几乎不需要太多更改。
对于单向 lstm 等自回归模型，只需每次计算保存隐状态值在输出给下个计算 chunk 即可。

#### CNN层
- 相对更复杂一些，需要格外小心 chunk 边界情况的处理
- 对于输入输出不等长的情况，可以考虑使用valid模式的卷积核，在边界处理时更简洁。可以参考wenet中subsampling 模块的实现。
- 大多数情况下，会在边缘处有冗余计算。chunk不太小的情况下，影响有限。


<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2021/2021-08-16-subsampling_overalp.gif?raw=true" width="500" /></div>



事实上，绝大多数非流式算法基础上，只需做少量的修改便可满足流式需求。
但是在模型设计时，要格外小心 chunk 间边界情况的处理。


要格外处理 chunk 之间边界情况的处理。
对于串行程序，可以采用这样的方案。

### 流式语音特征

参考wenet对流式特征的处理

#### wav acceptor

#### mel-spec

#### pitch or energy
 

### 流式神经网络模型

首先，需要看到全文的神经网络不能实现流式，例如双向lstm模型。
除此之外，如果模型只需要向后看有限帧，可以进行流式，不过可能会多花一些精力来重构推理的流程。


```text
# for offline
input = ...
x1 = model_a(input)
x2 = model_b(x1)
output = model_c(x2)

# for online
input_chunk = ...
input.append(input_chunk)  # add more data for queue of inp
x1_chunk = model_a(input[p_inp, p_inp + len_inp])  # compute chunk of inp
x1_valid = x1_chunk[0]
x1.append(x1_valid)  # add generate data for queue of x1
x2_chunk = model_b(x1[p_x1, p_x1 + len_x1])  #  compute chunk of x1
... 

```

下面针对几个具体的案例来看下流式操作是怎样实现的。

#### RNN and auto-regressive

循环神经网络和自回归模型由于自身特点，天然具有流式的特性。
改造工作非常简单，只要使用变量每次存储模型的隐状态，并作为下次模型的输入即可。

补充一个例子： rnn 声码器伪代码

 
#### CNN

卷积神经网络由于chuangchang的缘故，需要向后看若干帧。首先我们来看下著名开源项目wenet的处理方法。

插入链接-wenet对cnn模型的处理。

一个例子。

如果采用流式的方案，则可以用下列方法.

维护两个队列，一个是mels不断送入，另一个是signals，每次计算若干

todo: hifigan代码重构一下。


### 测试用例 (cpp)


## 瓶颈和挑战

- 简洁高效的实现，不要使用过于复杂的逻辑。
- 对边界对处理，不会带来额外的 badcase
- 多线程 or 多进程的处理技巧
