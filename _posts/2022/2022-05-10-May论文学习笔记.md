---
layout: post
title: 2022-05 May论文笔记
categories: 读书笔记
description: 
keywords: 语音信号处理
---

2022年5月-论文/读书笔记

## TTS and Vc

### A: [NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality](https://arxiv.org/pdf/2205.04421.pdf)

微软关于 tts 最新论文，使用 vae 的主体架构

核心的几个优势:
- 解决训练/推理之间的 mismatch
- 减缓 one-to-many 的问题
- 提升 representation 的表现能力

整个tts的系统的特点:
- 非自回归结构，推理速度快
- 纯端到端系统，非串联
- 可微分结构 (改进了不可微分的 priority encoder 和 durator)
- 更好的性能 (这个似乎讲得有点牵强) 

结构简图:

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-01.png?raw=true" width="600" /></div>

整体结构是一个标准的 vae 结构

wave: x, phoneme: y, representation: z

```math
x: -> z -> x`
z = q(z|x) 
y:-> z 
z` = p(z`|y)
loss = rec_loss(x`, x) + KL(z`||z)  
```
other trick: 
- 使用带有 mask 机制训练的 encoder，(类似bert？), 使用 adjacent phoneme 作为特征，
本质解决文本维度过高的问题
- 可微分的 durator，soft dynamic DTW for KL
- 基于 flow 的 prior/posterior 转换器，**这里不是直接对p和q求kl距离**，而是对转换后的特征求。
- 使用 vae 结果作为 decoder 的 attention。

---

### B: [SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech](https://arxiv.org/pdf/2204.11792.pdf)

浙大 tts 前端的工作

使用 Dependency Parsing Tree 来对文本前端进行分析，并提取对应特征。

---

### C: [ONE-SHOT VOICE CONVERSION FOR STYLE TRANSFER BASED ON SPEAKER ADAPTATION](https://arxiv.org/pdf/2111.12277.pdf)

西工大 one-shot vc

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-02.png?raw=true" width="600" /></div>

- speaker normalization，核心还是去除 speaker 信息，但是没怎么详细讲
- finetune 的时候对参数使用了 weight regularization，让参数不要偏移太多，来应对 one-shot 的情况，可能有用，待尝试？
- source style transfer 跟之前的论文差别不大
- 3-stage 训练

### D: [VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis](https://arxiv.org/pdf/2107.03298.pdf)

港大的 VAENAR-TTS 系统，感觉还是很经典的 vae 结构: 

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-05.png?raw=true" width="600" /></div>

- p(y|x) -> p(y|z, x)
- 同时训练两个 encoder，使得 KL(q(z|x, y), p(z|x)) 尽量小
- 使用 glow for priority(感觉没啥必要？？？)
- 另一个细节是我之前一直写 vae z 都是 utt level的，这篇论文是 frame-level 的，可能还是有点区别，可能要实现验证一下。

### E: [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis](https://arxiv.org/pdf/2205.07211.pdf)

浙大关于 tts 的最新进展，关于 tts style-transfer 的论文

主要在讲目前 tts 发展的两个难点:
1. transfer 到 ood(out-of-domain) 的难度。许多模型本质其实是在数据的驱动下，
学到对应的平均分布，缺乏精确可控的表达
2. data training/inference mismatch 造成效果的下降

实质上模型设计和大多数一样遵循的两个方案，一部分是 style 无关的，主要是 linguist content，
一部分是 style 相关的，后者是设计的核心，这里主要是设计了一套 multi-level style adaptor

模型设计层面感觉似乎说服力没有足够强。

## ASR

### A: [E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR](https://arxiv.org/pdf/2204.10749.pdf)

google Sad 结合 rnnt 的方案

- 使用音频和文本作为输入
- 使用规则来生成数据 heuristic-based weak supervision approach 

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-03.png?raw=true" width="400" /></div>

### B: [TINY TRANSDUCER: A HIGHLY-EFFICIENT SPEECH RECOGNITION MODEL ON EDGE DEVICES](https://arxiv.org/pdf/2101.06856.pdf)

tx 关于轻量级 tranducer 的文章

几个核心要点：
- phone-based tranducer + decoding graph 的结构
- blank deweight 的操作来减少 del error
- DFSMN + SVD 降低维度来进一步提速

几个有意思的结果
- 轻量级 DFSMN 在流式场景下的优势。
- 解码时候，对音素先验不做归一化。对 blank 值做 deweight，方法简单粗暴给静音帧都除一个常数，
剩下的再使用跳帧解码。
- 端侧的 int8 量化  

## ST

### A: [Cross-modal Contrastive Learning for Speech Translation](https://arxiv.org/pdf/2205.02444.pdf)

字节使用对比学习来做语音翻译的论文，本质是希望分别找到表征映射，将音频和文本映射到同一个表征空间里，使得content内容相同的音频和文本对的距离尽可能近。

几个主要的贡献点：

- 提出来基于多任务学习和对比学习的 ConST 框架
- 提升了业界 benchmark 的最好水平
- 方法确实可以学到一个更好的表征，效果不仅仅表现在 ST 的 BLEU 上

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-04.png?raw=true" width="400" /></div>

核心点在于 Ctr loss, 这个模型构造的方法还是有可以借鉴的地方

几个数据增强的方法：
- span-masked Augmentation for signal
- word repetition for text 
- cut-off strategy for signal feature(both row and col)

另外，比较了 ctr loss/ctc loss/ce loss 之间的对比差异等

实验比较完备和详尽。

---
