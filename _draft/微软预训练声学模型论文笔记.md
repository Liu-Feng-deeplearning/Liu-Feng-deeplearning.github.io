
# 预训练语音模型论文笔记 


微软 Wav2LM


## SuperB 

[Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051)

总结整理了语音领域里常见的下游任务，建立了对应的 leaderboard，后面可以让预训练模型 "刷榜"。

大体上分成识别和生成两大类任务。

**discriminative tasks**

**Content**
- Phoneme Recognition（音素识别）
- Automatic Speech Recognition（语音识别）
- Keyword Spotting（关键词检测）
- Query by Example Spoken Term Detection（QbE），在(不转录的情况下)从音频文件中检索对应 query 的任务 

**Speaker**
- Speaker Identification（说话人识别）
- Automatic Speaker Verification（说话人验证）
- Speaker Diarization（说话人日志）

**Semantics**
- Intent Classification（意图分类）
- Slot Filling（对话理解填槽）
(类似于语音场景下的 NLP 问题)

**Paralinguistics(副语言信息)**
- Emotion Recognition（情绪识别）

**generative tasks**
- Speech Separation（语音分离）
- Speech Enhancement（语音增强）
- Speech Translation（语音翻译）

非常遗憾的是，榜单上没有 tts/vc 这样的任务，大概是因为不太好用客观指标进行衡量？
分离和增强毕竟还可以获得一个稳定的评价指标。

### 缘由

### vc中模型解耦方案

论文1
论文2
论文3

### 预训练语言模型

promt

### wav2LM

https://www.zhihu.com/question/512946937/answer/2346189296

https://www.zhihu.com/question/512946937/answer/2346189296

### hubert训练方法
