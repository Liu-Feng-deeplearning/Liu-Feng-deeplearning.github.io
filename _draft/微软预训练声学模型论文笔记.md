
# 预训练语音模型论文笔记 


## SuperB 

[Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051)

总结整理了语音领域里常见的下游任务，建立了对应的 leaderboard，后面可以让预训练模型 "刷榜"。

大体上分成识别和生成两大类任务。

**discriminative tasks**

**Content**
- Phoneme Recognition（音素识别）
- Automatic Speech Recognition（语音识别）
- Keyword Spotting（关键词检测）
- Query by Example Spoken Term Detection（QbE），在(不转录的情况下)从音频文件中检索对应 query 的任务 

**Speaker**
- Speaker Identification（说话人识别）
- Automatic Speaker Verification（说话人验证）
- Speaker Diarization（说话人日志）

**Semantics**
- Intent Classification（意图分类）
- Slot Filling（对话理解填槽）
(类似于语音场景下的 NLP 问题)

**Paralinguistics(副语言信息)**
- Emotion Recognition（情绪识别）

**generative tasks**
- Speech Separation（语音分离）
- Speech Enhancement（语音增强）
- Speech Translation（语音翻译）

非常遗憾的是，榜单上没有 tts/vc 这样的任务，大概是因为不太好用客观指标进行衡量？
分离和增强毕竟还可以获得一个稳定的评价指标。

## FAIR Hubert
Huber: [Hidden unit Bert](https://arxiv.org/pdf/2106.07447.pdf)

#### abstract
提出了 speech representation 的三个问题:
1. 每个句子中包含多个发音单元
2. 预训练模型下，缺乏发音单元的对应字典
3. 发音单元长度不等，没有显示的分割界限

#### introduction
也是希望能够对音频进行解耦。不使用label信息，除了成本高外，另一个原因是label只包含了content信息，
丢掉了其他内容。

语音和其他领域的不同: NLP 问题有先验词表，是一个离散信息表示的问题。
CV 问题是一个instance classification 问题，可以通过对每个样本进行数据增强，得到这个领域的对应正例。
对应语音问题，因为摘要中提到的三点，导致 CV 和 NLP 的方法在此并不适用。

这篇文章目的也是从音频抽取隐层表示，从而使用类似Bert的方法进行训练：预测mask掉的标签。
论文重点强调，预训练的目标不仅仅是为了降低错误率，更为了学习目标的seq 结构表示。
各种不同尺寸模型效果对比。

#### Method
- MFCC 聚类得到 pseudo label，迭代两次
- cnn encoder + transformer: feat to hu (类似bert)
- Pretrain + finetune 的方案 (for asr)

#### Experient
- Data: 960 libriSpeech + 60000 Libri-light
- Cluster: MiniBatchKMeans in scikit-learn
- 使用 force-alignment 来评估 cluster 的效果
- 各种对比实验结果: 参照 Wenet-LibriSpeech 的实验，还是显著有效果的。

---

## Wav2Vec2.0

和 Hubert 模型结构非常像。几个额外有意思的点:

- Gumbel softmax，使得离散采样可求导和参与训练
(更专业的说法是参数化，可以参考[这篇文章](https://zhuanlan.zhihu.com/p/50065712))
- product quantization module，本质上还是从一个预先设定好的向量矩阵中进行检索，不过这个过程是参与训练的。
[关于product quantization的一个介绍](http://www.fabwrite.com/productquantization)

看上去 Hubert 和 Wav2Vec2.0 相比，最大的优势在于改进了隐层特征的获取方法。

---

## 微软 Wav2LM

[论文地址](https://arxiv.org/pdf/2110.13900.pdf)
 
看上去两个核心的突破
1. large scale,  94k hours 的训练数据，在 SuperB 大多子任务上都取得最好成绩
2. 解决 wav2Vec 和 Hubert 的一个原理上的弊端: 只针对识别类任务，对 content representation 进行抽取。
Wav2LM 希望能包含更多信息，从而在 non-ASR 上取得好的表现。即 for Full Stack Speech Processing
 
显然，第2点是本文更重要的突破，实际的操作，一言以蔽之是 mask + overlap-augmentation 

### Model
- 多引入了 gated relative position bias 模块，具体细节可参看原文，但我觉得可能不太重要。

### Experiement
- 针对每个下游任务的调优方法和实验结果


