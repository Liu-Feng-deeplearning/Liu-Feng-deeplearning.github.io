


# 深度学习模型调参经验技巧总结

接触深度学习四年来，大概工作中有六成以上的时间都在写模型/调模型/分析模型为什么好或不好。这个过程中总结来一些经验，
特此记录。

由于现在还在做相关工作，这篇文章可能后续还会不断更新。

### 切实有效的评价指标

一个例子，在 asr 开发工程中，经常会针对不同领域进行测试。假如我有六个不同的测试集，
我希望模型能在这六个测试集上都有好的表现。但领域不同导致数据集规模差别较大（例如模型领域只有很小但测试集）。
这个时候，每次迭代时，如果六个测试集中四个 wer 降低，我更新线上模型。

这样做对吗？

结论：是不对的。这个标准不具备传递性，很容易构造反例，出现A<B, B<C, C<A的情况。
从而导致可能迭代来几个周期后，模型效果反而不如最开始的版本了。

对于有明确指标的 asr 尚且如此，那对 TTS/DS/VC 这种以 mos 为主要评价指标的问题更需谨慎小心。

### 不要在测试集上调参

看上去道理简单通俗易懂，但不小心还是会经常犯错。

一个例子，在 asr 声学模型优化过程中，我有三个模型（代号 A/B/C），通过一个测试集进行判断，
选择 wer 最小的用于上线。 这是一个正确的做法吗？

结论：不正确。不同模型可以认为是不同的超参，犯了在测试集上调参的错误。
因此，不能仅凭一组实验，简单得出不同模型/方法之间的优劣。

### 方法应该对训练数据鲁棒

如果一个方法是有效的，那么应该对应用场景大部分数据有效。
否则，在 a 数据下， 方法A优于方法B，在b数据下，反之。
对于方法A进行一些随机更改便可实现，优化工作变得平凡。


### 注意模型本身的效果波动

一个例子，在 asr 声学模型优化过程中，wer指标，方法a优于方法b 0.05%, 但模型两次训练精度误差有0.5%, 
能说明模型但提升吗？不能。

了解结果的置信区间很重要（就像tts评分中总是尽量给出 mos 的置信区间。）

### 调参并不重要

很多时候，调参并不重要

### 小心特征泄漏