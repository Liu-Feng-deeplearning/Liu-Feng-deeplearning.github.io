---
layout: post
title: 2022-05 论文笔记
categories: 语音信号处理
description: 
keywords: 语音信号处理
---

陆续更新中

## TTS and Vc

### [NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality](https://arxiv.org/pdf/2205.04421.pdf)

微软关于 tts 最新论文，使用 vae 的主体架构

核心的几个优势:
- 解决训练/推理之间的 mismatch
- 减缓 one-to-many 的问题
- 提升 representation 的表现能力

整个tts的系统的特点:
- 非自回归结构，推理速度快
- 纯端到端系统，非串联
- 可微分结构 (改进了不可微分的 priority encoder 和 durator)
- 更好的性能 (这个似乎讲得有点牵强) 

结构简图:

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-01.png?raw=true" width="600" /></div>

整体结构是一个标准的 vae 结构

wave: x, phoneme: y, representation: z

```math
x: -> z -> x`
z = q(z|x) 
y:-> z 
z` = p(z`|y)
loss = rec_loss(x`, x) + KL(z`||z)  
```
other trick: 
- 使用带有 mask 机制训练的 encoder，(类似bert？), 使用 adjacent phoneme 作为特征，
本质解决文本维度过高的问题
- 可微分的 durator，soft dynamic DTW for KL
- 基于 flow 的 prior/posterior 转换器，**这里不是直接对p和q求kl距离**，而是对转换后的特征求。
- 使用 vae 结果作为 decoder 的 attention。

---

### [SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech](https://arxiv.org/pdf/2204.11792.pdf)

浙大 tts 前端的工作

使用 Dependency Parsing Tree 来对文本前端进行分析，并提取对应特征。

---

### [ONE-SHOT VOICE CONVERSION FOR STYLE TRANSFER BASED ON SPEAKER ADAPTATION](https://arxiv.org/pdf/2111.12277.pdf)

西工大 one-shot vc

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-02.png?raw=true" width="600" /></div>

- speaker normalization，核心还是去除 speaker 信息，但是没怎么详细讲
- finetune 的时候对参数使用了 weight regularization，让参数不要偏移太多，来应对 one-shot 的情况，可能有用，待尝试？
- source style transfer 跟之前的论文差别不大
- 3-stage 训练

### [VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis](https://arxiv.org/pdf/2107.03298.pdf)

港大的 VAENAR-TTS 系统，感觉还是很经典的 vae 结构: 

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-05.png?raw=true" width="600" /></div>

- p(y|x) -> p(y|z, x)
- 同时训练两个 encoder，使得 KL(q(z|x, y), p(z|x)) 尽量小
- 使用 glow for priority(感觉没啥必要？？？)
- 另一个细节是我之前一直写 vae z 都是 utt level的，这篇论文是 frame-level 的，可能还是有点区别，可能要实现验证一下。

## ASR

### [E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR](https://arxiv.org/pdf/2204.10749.pdf)

google Sad 结合 rnnt 的方案

- 使用音频和文本作为输入
- 使用规则来生成数据 heuristic-based weak supervision approach 

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-03.png?raw=true" width="400" /></div>

## ST

### [Cross-modal Contrastive Learning for Speech Translation](https://arxiv.org/pdf/2205.02444.pdf)

字节使用对比学习来做语音翻译的论文，本质是希望分别找到表征映射，将音频和文本映射到同一个表征空间里，使得content内容相同的音频和文本对的距离尽可能近。

几个主要的贡献点：

- 提出来基于多任务学习和对比学习的 ConST 框架
- 提升了业界 benchmark 的最好水平
- 方法确实可以学到一个更好的表征，效果不仅仅表现在 ST 的 BLEU 上

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2022/2022-05-10-paper-img-04.png?raw=true" width="400" /></div>

核心点在于 Ctr loss, 这个模型构造的方法还是有可以借鉴的地方

几个数据增强的方法：
- span-masked Augmentation for signal
- word repetition for text 
- cut-off strategy for signal feature(both row and col)

另外，比较了 ctr loss/ctc loss/ce loss 之间的对比差异等

实验比较完备和详尽。



---

## To Read List

### [Voice Conversion Based Speaker Normalization for Acoustic Unit Discovery](https://arxiv.org/pdf/2105.01786.pdf)
### [A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes](https://arxiv.org/pdf/2204.06164.pdf)
### [Unified Speech-Text Pre-training for Speech Translation and Recognition](https://arxiv.org/pdf/2204.05409.pdf)
### [VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration]()
