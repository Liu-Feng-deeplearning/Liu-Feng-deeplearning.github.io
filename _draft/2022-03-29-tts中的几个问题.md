
# TTS 中的过度平滑问题-VAE 与 GAN

最近读了一下 [Revisiting Over-Smoothness in Text to Speech](https://arxiv.org/pdf/2202.13066.pdf)
，感觉还是有所启发。同时随手记录一些最近思考得 tts/vc 中表现力相关的问题点，无论对错，留个痕迹未来再回顾。

### NAR (non autoregressive) 的缺点

现在来看，NAR 几乎是 tts/vc 模型的标配了。今年在 icassp 上读过了好几篇论文的贡献点都在将在即模型的 NAR。
但 NAR 真的更好吗？ 虽然不一定更好，但更快是一定的。更少的计算量和天然可并行的架构，在工程性能上具备相当客观的有时。
一个例子是 vocoder，从 wavernn 到 melgan/hifigan 几乎一夜之间，很快后者几乎就成为了 vocoder 标配。NAR + GAN 的结构，
效果好且快，而且常见的mode collapse 和over smooth 的问题却并没有发生。

NAR 和 AR 最大的区别是后者用前一时刻/上一像素点的输出结果作为输入，来完成生成任务。
对于绝大多数生成任务来讲，都是一对多的映射关系。例如一个等长音频的映射 x -> y, 
x[0:5] -> y3, x[1:6] -> y4 这个时候，可能会存在 x -> y/y` 两种情况，可能 y=[... y3, y4 ...] / [... y'3, y'4] 都是符合预期的结果。
但由于训练但不稳定性，可能生成 [... y3, y'4 ...] 这样的 badcase，或者生成 [... (y3 + y'3) / 2, y4 ...] 进而造成效果的下降。
后者就是典型的 over-smooth 问题（blurry）

类似的情况，也出现在 asr 领域。rnnt Vs wenet，后者为了解决类似的问题，引入了 ctc loss后，
使用 attention-based 来进行二次打分和重排，就是为了尽量降低没有之前文本信息带来效果的损失。
  
### 过度平滑(OverSmooth)

过度平滑是语音合成中经常出现的问题，具体现象是生成音频的风格表现力变弱，生成音频感觉会更平淡，
从而会丧失一些发音人的特点，更严重的发音会糊掉。特别是在 emotional tts 这个细分领域，可能出现的概率会更频繁。

过度平滑出现的原因是什么？对于大多数 tts 系统，训练的核心都是 使用 MSE/MAE 来最小化重建损失(reconstruction loss)
来获得自然度和音色相似度的最优。即对于训练数据 $x, y$代表文本和音频，训练模型 &y` = f(x), s,t, loss(x, y) &

但是，和文本数据而言，音频数据其实是更为稠密的数据表现形式。
- 相同的文本信息，可能会存在不同的音频，这些音频对应的波形 y可能数据分布差异很大
- 文本信息只是音频 content 的高度抽象的表达，除此之后，还有音色/duration/prosody等其他
音素会影响整个音频。

因此如果观察数据分布，往往会存在同一个 x 对应多个 y 的情况。
即使我们通过合适的筛选数据保证 utterance 的程度不存在，但 level 层面依然有。
毕竟 text 里一定会有大量的音素标签反复出现。

假如对于某个x，这里以 ang1 为例（汉语中张的韵母），会存在多个y，如果画出这些y的分布，
大概率是概率分布。如果是 mse 方法训练，大概率x 会对应到这些 y 分布的平均值上。
这便是过度平滑造成损失的来源。

对于传统的 merlin 方法，首先训练 duration model 后，将每个音素对应到对应的帧上面，因此
过度平滑出现非常频繁。但是对于tacotron-based 方法，存在 encoder 将 x 转成 enc-embed。
encoder(cbhg/transformer) 一般是接收整个文本作为输入，通过全局 attention 和 双向 rnn,
这样即使对应相同音素的相邻帧，也会有不同的特征表示。再加上自回归的decoder，可能会更大程度上规避这样的问题。

但是，相似的文本依然有可能得到相似的 embed。


之前提到的 Revisiting 论文，就是重点在分析和解决这个问题。整体而言两个思路:
使用更多 variance data 以及模型层面的改进

之前我们提到过 x - y 容易出现 一对多的训练情况，我们可以通过扩展数据，增加 x 的维度，从而训练
变成了 x|x1 -》y ，从而简化了y的数据分布，进而解决问题。

例如 tacotron 和 merlin 相比，一个重大的进步就是，使用 nnet-encoder 提取 embedding，这个过程中，
每帧 embed 不仅和当前音素有关，还和之前/之后的音素有关。某种程度上变相的使用了更多信息。

还有一些更为先进的方案，采用了dur/pitch/energy 的特征，本质上都是"细化"了x, 从而简化 了y
的分布。

除此之外，还有一些模型层面的优化方案，例如改进损失函数和结构。
前面我们提到过的自回归方案，就是一个不错的选项，当然他自己还有一些别的问题，例如性能。
使用 Flow/VAE/GAN 之类的方案来做。

题外话: 我最开始接触 tts 这个领域是客服场景，开始拿到的数据中存在大量这种一对多的情况，
当时会发现相同模型在不同数据上效果差别很大。具体表现是bzn 训练效果和稳定性都非常好，但切换到自有数据上效果会下降。
下降的表现点是发错音的频率增加以及清晰度下降（通俗讲就是糊了）。
现在复盘来看，这两个gap 可能都和原始数据中存在过多 one-to-many 有关。

### VAE 

vae(variable encoder) 是目前生成问题中大火的模型。看了一些文章和介绍，
但感觉有点还是不够清晰。

vae 的核心是认为 encoder 由很多个隐变量组成，每个隐变量分布虽然未知，
但总是可以用多个高斯分布的和近似模拟。

例如从 数字 label -》 图片（minst经典识别的反问题）
可能除了 content以外，图片中还包括了 style/position/灰度等其他的信息。

训练流程：

推理流程：

这些信息 都用隐变量 来表示，但可能会。

另一个缺点是可解释性。

**vae-vc**      

可能并不是很适合。

但是 对于 vc 而言，


### GAN

GAN 也是目前比较主流的生成方案。GAN-based vocoder 目前已经成为了主流声码器。
和其他方法相比，gan 主要可以大幅度提升生成样本的清晰度（特别是针对 non-auto gressive）。

但是 gan 面临的一个理论的缺陷是 mode collapse

**mode collapse**

Mode collapse 问题：如果 x -》y，也是一个 1对多的问题，gan 会随机的收敛到其中一个值上
（不是平均值），虽然会更好一点，但也会丢掉很多其他可能的情况。

然而比较幸运的是，对于声码器来说，x（mel）本身也是稠密的，几乎是一个1对1问题，
这和图片生成/tts生成有很大的区别。因此 Mode collapse 非常不严重。信号值虽然更稠密，但mel
几乎包括了所有但信息。

除此之外，还有一类方法是在 mel 谱层面使用 Gan，从这个角度看，似乎必要性不大。
