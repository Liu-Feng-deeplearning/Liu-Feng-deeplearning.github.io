---
layout: post
title: 音频指纹原理与优化 
categories: 语音信号处理
description: 
keywords: 语音信号处理
---

# 音频指纹算法原理


该算法在相当程度上借鉴了 dejavu 算法库([python 版本的实现](https://github.com/worldveil/dejavu))，
其原理可以参考[该文](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)

这里侧重讲下本算法在此基础上的改进。

### 编码、采样与频谱提取

虽然大部分音乐的采样率都是 44100 Hz，但我们统一将音频降采样到 8000 Hz，这样我们只需要专注该case，
不需要去考虑类似"测试音频采样率小于anchor音频"的情况。事实上，8kHz 的采样率对于该问题足够了。


和大多数音频问题一样，我们选择在频谱上处理该问题。我们使用 fft 对每个小窗进行频谱提取，
但我们不会像其他语音问题那样，选择fbank or MFCC。我们选择 1024 和 80 作为帧长和帧移。
前者但原因是为了更快进行傅立叶变换，后者是希望能够在较细粒度上捕捉到频谱特征，从而获得更高的准确率。

### 指纹提取
 
对于音频来说，我们和 dejavu 一样，选取了频谱上出现最大值的位置（而非最大值本身）作为音频指纹特征。
但我们采样了另一种方式来处理数据。

dejavu 使用 scipy 寻找频谱上的 2D peaks，合适的mask选取，使得相邻peaks之间的间距不会太近。同时，
dejavu 抹掉了绝对值过低的peaks。下图是一个例子，可以看出某些静音段可能没有对应的peaks。

<div align=center><img src="freq.png?raw=true" width="600" /></div>


获得peaks后，dejavu 选择距离不太远的一组 pairs，保存两个点的freq和时间轴上相对距离作为 fingerprint。
如下图每条黑线两端的点，组成一个 fingerprint，


例如 (318, 119), (350, 119) -> (318, 350, 0) or (350, 119), (330, 142) -> (350, 330, 23)

<div align=center><img src="freq2.png?raw=true" width="400" /></div>


相对的，我们的方案更简单。对每帧的频谱进行子带划分，提取每个子带最大值出现位置。
例如我们将1024维的频谱划分成6个子带：0～100，100～200，200～300，300～400，400～500，500～1024，
每帧的指纹便是一个6 维向量。例如 [5,123,201,370,423,599]

注: shazam 也用了类似的逻辑，只是子带划分的方法有所不同。

### 指纹比对

dejavu 方法及其他一些开源的方法，将识别音频使用同样的方法提取指纹，并与指纹库进行比对。
选择匹配最多的指纹特征。如果指纹中储存了时间轴绝对信息，也可以根据此获得待识别片段在anchor音频上的时间戳信息。

该方法有一个很大的问题：每个指纹包含信息太少极容易造成触发。如果指纹包含信息太多，又很容易匹配不上，造成漏检。
噪声or音频重放采集会显著加剧漏检的概率。

我们采样了一种"投票"的机制，以期望获得更好的鲁棒性。我们获得 target 音频每帧的指纹，将其和每个 anchor 中每一帧进行匹配，
这里我们设计了一种模糊匹配的方式，来代替精确相等匹配。同一帧可能匹配到 anchor 里的多个帧。
例如下图，
```text
frame6 -> anchor1 frame1(a1fr1)
frame7 -> a1fr2
frame8 -> a1fr4
frame9 -> a1fr4, a1fr5, a2fr3
```
<div align=center><img src="voting.png?raw=true" width="400" /></div>

每个匹配帧，都可以反推一个时间开始的位置。对多个时间结果进行投票，结果最多的即为最终结果。
例如，之前的例子下，匹配音频为anchor1， shift为5帧，即anchor1的第0帧和target的第5帧重合。

```text
frame6 -> a1fr1 -> anchor1 shift5(a1sh5)
frame7 -> a1fr2 -> a1sh5
frame8 -> a1fr4 -> a1sh4
frame9 -> a1fr4, a1fr5, a2fr3 -> a1sh5, a1sh4, a2sh6

voting:
a1fr4: 2, a1fr5: 3, a2sh6:1
```

实验中观察发现，对于大部分case，我们观察匹配的anchor帧下面，每帧对应的票数呈正态分布
（正态分布的顶点横坐标即为anchor-target的偏移值），而非匹配的anchor帧下面，每帧票数呈均匀分布。
这意味着，大多数噪声，带来的影响只会使正态分布的曲线变缓，而不会改变峰值位置。
因此，该算法基于大量数据（这也是我们为什么选择10ms作为帧移的原因）产生的投票结果可以做到更鲁棒的原因。
       
### 存储与搜索

一个成熟的指纹算法，势必要考虑到海量 anchor 带来的存储问题，以及在大规模数据中搜索的时间复杂度。
目前我们的算法并未做过多这方面的设计和考虑。

### Streaming input

观察这个算法的过程，会发现所有的操作，都是针对单一帧完成的，并不涉及不同帧之间的相互关系。
这使得我们很容易将该算法改成 streaming-mode 模式，每次接收当前帧进行判断。

在很多场景，我们希望尽量早的获得结果。例如，对于某个anchor-target，两者在第5s位置发生匹配。
显然在第6s返回结果，会比第10s返回结果更好。为了能够更早的返回结果，我们使用了一个 trick: 
不要等正态曲线分布完全形成再出结果，而是人为手工设计一个阈值，当某帧投票值超过该阈值时，立即返回当前结果。

实际实现时，会遇到另一个工程层面的问题是，当 target 音频非常非常长的时候，可能会导致整个投票落在一个非常广的
区间上，造成计算资源的浪费。我们采用了滑动窗，滑动窗以外的结果我们不予考虑，这样大幅度提高了计算效率。在 cpp 实现中，
借助优先队列，还可以对性能做进一步提升。

### 丢帧和乱序

丢帧和乱序可能是只在流场景下可能会遇到的问题。因为算法只针对单一帧进行操作，所以并不需要前后帧的相对位置关系，
只需输入当前帧的特征和顺序id即可。因此，算法天然对乱序具备鲁棒性。对于丢帧，只需有足够多的有效帧存在即可。

一个极端的case，一个音频流中每三帧随机丢掉其中两帧，dejavu 的帧相对位置关系大多数会被破坏从而导致失效，
但我们的算法不会。

已经在实际的项目中，验证了我们算法的鲁棒性。

### 效果对比 Ours Vs dejavu

我们采用了项目 https://github.com/mimbres/neural-audio-fp 中提到的数据 Dataset-mini v1.1 (11.2 GB) 进行测试。
测试样本一共有486条，每条样本 30s 作为 anchor, 选择前**5s**作为 待检测音频。测试结果如下

- hit rate: 检测成功率
- detect time: 检测所用时间

hit rate| ours | dejavu 
:---:|:---:|:---:
486 items| 0.94|0.64


total time[s]| ours | dejavu 
:---:|:---:|:---:
486 items|11300s|162s

看上去，我们方法的准确性更高，但是耗时显著更长（这可能也是接下来一个亟待解决的问题）

备注: dejavu 的结果是基于开源项目 https://github.com/worldveil/dejavu 进行改写
在保持结构和参数不动的前提下，使用python dict代替了数据库的基本操作。改写并不能一定确保效果上无损失，
但和论文 [NEURAL AUDIO FINGERPRINT](https://arxiv.org/pdf/2010.11910.pdf) 中对 dejavu 引用结果大体一致。


## 音频指纹相关 ref
- http://coding-geek.com/how-shazam-works/
- https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition
- https://github.com/itspoma/audio-fingerprint-identifying-python
- https://github.com/worldveil/dejavu
-  [NEURAL AUDIO FINGERPRINT FOR HIGH-SPECIFIC AUDIO RETRIEVAL BASED ON CONTRASTIVE LEARNING](https://arxiv.org/pdf/2010.11910.pdf)