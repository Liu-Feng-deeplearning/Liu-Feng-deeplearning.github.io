---
layout: post
title: 高性能计算学习笔记(CS267)
categories: 高性能计算
description: 
keywords: 深度学习，高性能计算
---
最近假期在家，抽空又过了一遍 cs267 ， 虽然高性能计算相关知识平时工作项目中经常用到，
但在实践基础上重新学一下理论，有助于知识体系结构的系统化和条理化，感觉还是很有收获。

课程地址 https://sites.google.com/lbl.gov/cs267-spr2021

记录了 lecture 核心要点作为此 post，后续陆续更新。

update:
- 20210823: Lec4~6
- 20210819: Lec1~3

---
## Lec1～3 基础知识

**Lec1**

Overiew

**Lec2**

- 寄存器简介，cache-hit and cache-miss
- 不同级别的cache速度差异
- SMP/DMP/SIMD 简介（貌似也是贯穿整个课程的）
- FMA 介绍（一次乘法+一次加分）为什么要单独提出这个概念？？ 

Example: gemm/gemv
- 提出概念 计算次数 and 访问内存次数
- 重点关系 matrix storage
- blocking or tiling for resgister 矩阵分片，用以降低读取数据的消耗

Example: conv
- 卷积核读到临时变量中，避免preload消耗. 如下，使用临时变量会更快。

```text
res += filter[0] * signal[0] // using filter value at slow memory

float f0 = filter[0]
res += f0 * signal[0] // using temp value at fast memory(register)
```

**Lec3** 

- recurrent multiply method
- strassen  method (算法层面的优化)

roofline model介绍: 
- 计算效果既和算力大小有关，也和访存带宽有关。
- 神经网络模型推理速度，既和参数量大小有关，也和flops有关。两者共同决定了瓶颈。

<div style="text-align: center"><img src="https://github.com/Liu-Feng-deeplearning/Liu-Feng-deeplearning.github.io/blob/master/images/posts/2021/2021-08-21-hpc-roofline.png?raw=true" width="500" /></div>

---

## Lec4～6 OpenMp

**Lec4**

- pthread 及各种锁，mutex的用法。知乎上一个讲的不错的文章，可以用来作补充材料。https://zhuanlan.zhihu.com/p/112297714
- OpenMp 简介 and usage
- SMP-share memory program 的实现
- False Sharing 介绍和解决办法


False Sharing 是很多情况下并行不起作用的根源:

L1 cache line 有大小（64bytes）。在同一个cache line的不同变量不是独立的，系统对整个cacheline有标签。
所以必行计算时，如果不同的变量位于同一个 cache line内，会发生数据竞争。
帮助理解的文章：https://zhuanlan.zhihu.com/p/85984250

这章结合数值积分的例子，包括了大量 OpenMp 的例子。需要在实践中多练习，
才能逐渐掌握里面的各种 trick 

**Lec5**

一些并行计算的例子例子简介

**Lec6**

结合 n-body 问题和 PDE 问题具体介绍。

感觉上，Jim Demmel 好像更偏传统工程学一些，他的课程部分里没有讲和神经网络相关的部分。

--------------------------------------------------------------------------------
## Lec7～Lec8 GPU and cuda

Cpu和gpu架构上的区别
gpu的基本简介